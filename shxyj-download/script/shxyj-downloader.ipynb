{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2165da88-dd3d-4f31-8475-1437378935fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 下载《社会学研究》期刊文章，并整理逐期、逐页目录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4805c-1bad-43bf-a043-71a2ab12ba3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 导入所需的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a13d2f-361d-41a9-9057-995ab273b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d6801-009e-4196-9986-249efdadc9f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 给出文件存储目录、常用函数和webdriver的基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c1c3ba-1284-433f-b5e0-cb344c2bf27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '../data/'\n",
    "datapath = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "648fb917-ae7c-439a-a515-e11039dcc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, file, outpath,suffix=\"\"):\n",
    "    # 创建最终的文件名\n",
    "    filename = f\"{file}{suffix}\"\n",
    "\n",
    "    # 存为 Excel 文件（使用 utf-16 编码）\n",
    "    excel_filepath = os.path.join(outpath, f\"{filename}.xlsx\")\n",
    "    df.to_excel(excel_filepath, index=False, encoding='utf-16')\n",
    "\n",
    "    # 存为 Python 二进制文件（Pickle 格式，也使用 utf-16 编码）\n",
    "    pickle_filepath = os.path.join(outpath, f\"{filename}.pkl\")\n",
    "    df.to_pickle(pickle_filepath, protocol=5)  # protocol=5 表示使用最高的 Pickle 协议版本\n",
    "    \n",
    "    #print(excel_filepath, 'saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e18076-8189-4028-aa97-09156e9af6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a3d6f8-b6ab-424f-8edd-db21973dc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver输入参数\n",
    "options=webdriver.ChromeOptions()\n",
    "out_path=os.path.abspath(outpath)\n",
    "prefs={'profile.default_content_settings.popups': 0, 'download.default_directory': out_path}\n",
    "options.add_experimental_option('prefs', prefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817aa88b-e136-4686-ad4d-db857a17cd90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 下载指定年份的期刊目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aedbc5-f4c3-407e-b1d0-316961d6ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '../data/issue_page/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efd30d-5b0b-4829-9622-65f8340fc2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(prompt):\n",
    "    for _ in range(5):\n",
    "        year = input(prompt)\n",
    "        now = time.strftime(\"%Y%m%d_%H%M%S\")[:4]\n",
    "        if re.match(r\"^\\d+$\", year) and int(year) >= 1986 and int(year) <= int(now):\n",
    "            return int(year)\n",
    "        else:\n",
    "            print(\"无效的年份！请重新输入：\")\n",
    "    print(\"输入年份超过五次错误，程序终止！\")\n",
    "    exit()\n",
    "\n",
    "def year_range():\n",
    "    now = time.strftime(\"%Y%m%d_%H%M%S\")[:4]\n",
    "    start_year = get_year(''.join([\"请输入起始年份（1986 <= start yr <=\",now,\"）：\"]))\n",
    "    end_year = get_year(''.join([\"请输入结束年份（1986 <= end yr <=\",now,\"）：\"]))\n",
    "\n",
    "    # 检查结束年份是否有效\n",
    "    while end_year < start_year:\n",
    "        print(\"结束年份必须大于或等于起始年份！\")\n",
    "        end_year = get_year(\"请输入结束年份：\")\n",
    "    return start_year, end_year\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602296c5-41fa-471d-9178-61258dcd3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year, end_year = year_range()\n",
    "print(\"起始年份：\", start_year)\n",
    "print(\"结束年份：\", end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647ec52-8f19-4f48-ab15-1723a76c3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化logging模块\n",
    "logging.basicConfig(filename=outpath+'download_'start_year+'_'+end_year+'.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# 记录脚本开始运行的时间\n",
    "start_script_time = time.time()\n",
    "logging.info(\"Script started\")\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "\n",
    "successful_pages = []\n",
    "failed_pages = []\n",
    "\n",
    "for yr in range(start_year, end_year + 1):\n",
    "    for issue_num in range(1, 7):\n",
    "        issue_url = f\"http://shxyj.ajcass.org/Magazine/?Year={yr}&Issue={issue_num}\"\n",
    "        \n",
    "        try:\n",
    "            driver.get(issue_url)\n",
    "            try:\n",
    "                driver.find_element_by_css_selector(\"table#tab tr\")\n",
    "                logging.info(f\"Page for Year {yr}, Issue {issue_num} has content.\")\n",
    "                successful_pages.append(issue_url)\n",
    "            except NoSuchElementException:\n",
    "                logging.warning(f\"Page for Year {yr}, Issue {issue_num} does not have content.\")\n",
    "                failed_pages.append(issue_url)\n",
    "                    \n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'html')))\n",
    "            \n",
    "            issue_content = driver.page_source\n",
    "            fl_name = f\"{yr}_{issue_num}\"\n",
    "            with open(f'{outpath}/{fl_name}.html', 'w', encoding='utf-8') as file:\n",
    "                file.write(issue_content)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to download the page for Year {yr} Issue {issue_num}. Error: {e}\")\n",
    "            failed_pages.append(issue_url)\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# 记录脚本结束运行的时间\n",
    "end_script_time = time.time()\n",
    "logging.info(\"Script ended\")\n",
    "\n",
    "# 计算脚本运行总时长\n",
    "total_time = end_script_time - start_script_time\n",
    "logging.info(f\"Total script run time: {total_time} seconds\")\n",
    "\n",
    "# 记录成功和失败的页面\n",
    "logging.info(f\"Successful pages: {successful_pages}\")\n",
    "logging.info(f\"Failed pages: {failed_pages}\")\n",
    "\n",
    "# 记录开始和结束的年份\n",
    "logging.info(f\"Start year: {start_year}, End year: {end_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7040996-dc69-4d5b-86d4-c5dc419aa28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缺少1996 issue-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b22d6-552b-419f-bdf5-acb2f2a426e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 清理出期刊总目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10599c4b-3eb5-4621-afa8-f592f4b41a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '../data/issue_page/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd68490-6b35-4778-940c-2a8be65e5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articles_to_dataframe(html_file_path):\n",
    "    # 读取HTML文件\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    data = []\n",
    "\n",
    "    # 解析HTML，提取文章信息\n",
    "    for tr in soup.select('table#tab tr'):\n",
    "        td = tr.find_all('td')\n",
    "        if len(td) > 1:\n",
    "            ul = td[1].find('ul')\n",
    "            if ul:\n",
    "                li_items = ul.find_all('li', recursive=False)\n",
    "                if len(li_items) >= 4:\n",
    "                    title = li_items[0].text.strip()\n",
    "                    abstract = li_items[1].text.strip()[len('[摘要]'):].strip() if li_items[1].text.startswith('[摘要]') else '无摘要'\n",
    "                    author = li_items[2].text.strip().replace('作者：', '').replace('\\n', ', ').strip()\n",
    "                    views = li_items[3].text.strip().split(' ')[-2] if li_items[3].text.strip().split(' ') else '0'\n",
    "                    article_url = li_items[0].find('a', href=True)['href'] if li_items[0].find('a', href=True) else '无文章链接'\n",
    "\n",
    "                    data.append([title, abstract, author, views, article_url])\n",
    "\n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(data, columns=['标题', '摘要', '作者', '浏览次数', '文章URL'])\n",
    "    \n",
    "    # 提取ID并更新URL\n",
    "    df['ID'] = df['文章URL'].str.extract(r'(\\d+)$')\n",
    "    df['文章URL'] = 'http://shxyj.ajcass.org/' + df['文章URL']\n",
    "\n",
    "    # 调整列顺序\n",
    "    cols = ['ID'] + [col for col in df if col != 'ID']\n",
    "    df = df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe6329-2382-4383-9c8c-6b02a1df38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收集目录中所有的HTML文件名\n",
    "files = [f for f in os.listdir(outpath) if f.endswith('.html')]\n",
    "\n",
    "# 提取年份和期数，转换为整数，用于排序\n",
    "files_sorted = sorted(files, key=lambda x: [int(part) for part in x.split('.')[0].split('_')])\n",
    "\n",
    "# 初始化一个空的DataFrame来存储所有数据\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# 按排序后的顺序遍历和处理每个文件\n",
    "for filename in files_sorted:\n",
    "    file_path = os.path.join(outpath, filename)\n",
    "    year, issue = filename.split('.')[0].split('_')\n",
    "    df = extract_articles_to_dataframe(file_path)\n",
    "    df.insert(1, '年份', year)\n",
    "    df.insert(2, '期数', issue)\n",
    "    all_data = pd.concat([all_data, df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20e8df-5c2f-466f-974f-c750efef1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(all_data, \"master_index\", outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a5a8c-1dbf-4ea3-ab75-5f75845335cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3171b0d-8734-4bb8-b2f3-d018742c6f15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 添加每篇文章的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9d637-6907-4018-a18e-d77c176a171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../data/issue_page/'\n",
    "outpath = '../data/article_page/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69ae06-3a72-445f-ac0c-d71239f7d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = datapath+'master_index.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c1e7c8-b866-42b8-a3fe-fe0afa6300f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 下载每篇文章的介绍页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8057a0-70fc-4c24-8571-d7bcba4b82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置日志配置\n",
    "logging.basicConfig(filename=outpath+'article_info.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 记录起始时间\n",
    "start_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "logging.info(f\"开始执行下载任务，起始时间: {start_time}\")\n",
    "\n",
    "# 初始化 Selenium WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "\n",
    "# 创建列表来存储下载失败的文章ID\n",
    "failed_articles = []\n",
    "\n",
    "# 遍历 DataFrame\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"下载进度\"):\n",
    "    # 获取 ID 和文章 URL\n",
    "    article_id = row['ID']\n",
    "    article_url = row['文章URL']\n",
    "    \n",
    "    try:\n",
    "        # 使用 Selenium 下载网页\n",
    "        driver.get(article_url)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'html')))\n",
    "        article_content = driver.page_source\n",
    "        \n",
    "        # 保存网页内容到文件\n",
    "        with open(f'{outpath}{article_id}.html', 'w', encoding='utf-8') as file:\n",
    "            file.write(article_content)\n",
    "        \n",
    "        # 记录成功下载\n",
    "        logging.info(f\"成功下载文章 {article_id}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # 记录下载失败\n",
    "        logging.error(f\"下载文章 {article_id} 失败: {e}\")\n",
    "        # 将失败的文章ID添加到列表中\n",
    "        failed_articles.append(article_id)\n",
    "    \n",
    "# 关闭 WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# 记录截止时间\n",
    "end_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "logging.info(f\"下载任务执行完成，截止时间: {end_time}\")\n",
    "\n",
    "# 记录统计信息\n",
    "logging.info(f\"总文章数: {len(df)}\")\n",
    "logging.info(f\"成功下载文章数: {len(df)-len(failed_articles)}\")\n",
    "logging.info(f\"下载失败文章数: {len(failed_articles)}\")\n",
    "\n",
    "# 记录所有失败的文章ID\n",
    "if failed_articles:\n",
    "    logging.warning(f\"以下文章下载失败: {failed_articles}\")\n",
    "else:\n",
    "    logging.info(\"所有文章下载成功。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c7bd8-bb40-40d3-ba28-76d3595e6e81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 继续提取信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad589f6-7601-45df-8150-bb694af491b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数来提取网页中的信息\n",
    "def extract_info(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # 提取英文标题\n",
    "        english_title = soup.select_one('td:contains(\"英文标题\") + td').text.strip()\n",
    "\n",
    "        # 提取英文摘要\n",
    "        english_abstract = soup.select_one('td:contains(\"英文摘要\") + td').text.strip()\n",
    "\n",
    "        # 提取作者单位\n",
    "        author_affiliation = soup.select_one('td:contains(\"作者单位\") + td').text.strip()\n",
    "\n",
    "        # 提取期刊\n",
    "        journal = soup.select_one('td:contains(\"期刊\") + td a').text.strip()\n",
    "\n",
    "        # 提取年.期:页码\n",
    "        issue_info = soup.select_one('td:contains(\"年.期:页码\") + td').text.strip()\n",
    "\n",
    "        # 提取中图分类号\n",
    "        classification = soup.select_one('td:contains(\"中图分类号\") + td').text.strip()\n",
    "\n",
    "        # 提取关键词\n",
    "        keywords = soup.select_one('td:contains(\"关键词\") + td').text.strip()\n",
    "\n",
    "        # 提取英文关键词\n",
    "        english_keywords = soup.select_one('td:contains(\"英文关键词\") + td').text.strip()\n",
    "\n",
    "        # 提取项目基金\n",
    "        project_fund = soup.select_one('td:contains(\"项目基金\") + td').text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting information from {file_path}: {str(e)}\")\n",
    "        #return {key: '-1' for key in ['英文标题', '英文摘要', '作者单位', '期刊', '年.期:页码', '中图分类号', '关键词', '英文关键词', '项目基金']}\n",
    "    \n",
    "    try:\n",
    "        # 提取下载链接\n",
    "        download_link = 'http://shxyj.ajcass.org' + soup.find('a', href=re.compile(r'^/Admin/UploadFile/'))['href']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting information from {file_path}: {str(e)}\")\n",
    "        #return {key: '-1' for key in ['英文标题', '英文摘要', '作者单位', '期刊', '年.期:页码', '中图分类号', '关键词', '英文关键词', '项目基金']}\n",
    "    \n",
    "    try:\n",
    "        # 创建一个字典用于构建DataFrame\n",
    "        info_dict = {\n",
    "            '英文标题': english_title if 'english_title' in locals() else '-1',\n",
    "            '英文摘要': english_abstract if 'english_abstract' in locals() else '-1',\n",
    "            '作者单位': author_affiliation if 'author_affiliation' in locals() else '-1',\n",
    "            '期刊': journal if 'journal' in locals() else '-1',\n",
    "            '年.期:页码': issue_info if 'issue_info' in locals() else '-1',\n",
    "            '中图分类号': classification if 'classification' in locals() else '-1',\n",
    "            '关键词': keywords if 'keywords' in locals() else '-1',\n",
    "            '英文关键词': english_keywords if 'english_keywords' in locals() else '-1',\n",
    "            '项目基金': project_fund if 'project_fund' in locals() else '-1',\n",
    "            '下载链接': download_link if 'download_link' in locals() else '-1'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting information from {file_path}: {str(e)}\")\n",
    "        return {key: '-1' for key in ['英文标题', '英文摘要', '作者单位', '期刊', '年.期:页码', '中图分类号', '关键词', '英文关键词', '项目基金']}\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16899204-1ee1-4f60-a325-79e6e58e2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 logging\n",
    "logging.basicConfig(filename=outpath+'Article_Index_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 设置 tqdm 进度条\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# 创建一个列表，用于存储每篇文章的信息\n",
    "articles_info = []\n",
    "\n",
    "# 遍历文件夹中的网页文件\n",
    "for article_id in tqdm(df['ID'], desc='Processing Articles'):\n",
    "    file_name = f'{article_id}.html'\n",
    "    file_path = os.path.join(outpath, file_name)\n",
    "\n",
    "    # 提取信息并添加到列表中\n",
    "    info_dict = extract_info(file_path)\n",
    "    info_dict['ID']= article_id\n",
    "\n",
    "    if info_dict:\n",
    "        articles_info.append(info_dict)\n",
    "\n",
    "# 将列表转换为DataFrame\n",
    "articles_info_df = pd.DataFrame(articles_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6a0d5-8072-474a-b99e-8493e3a4a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将两个DataFrame连接在一起，按照原有的df['ID']列连接\n",
    "merged_df = pd.merge(df, articles_info_df, on='ID', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10236acc-c078-48ba-ac12-dfc12549d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动修正出错的单元格\n",
    "merged_df.loc[merged_df['ID'] == '72349', '英文标题'] = \"State , Market and Society: The multidimensional impetus of reducing the Qinhuai River's pollution\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '英文摘要'] = \"Since 1950, the Qinhuai River in Nanjing city has undergone five times of large-scale renovations. ... (以下是英文摘要的修改内容)\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '作者单位'] = \"南京大学社会学系 上海高校社会学E-研究院(上海大学)\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '期刊'] = \"社会学研究\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '年.期:页码'] = \"2008.1:143-164\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '中图分类号'] = \"X321\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '文章编号'] = \"\"  # 更新文章编号为空字符串\n",
    "merged_df.loc[merged_df['ID'] == '72349', '关键词'] = \"国家； 市场与社会； 污染治理； 中国特色；\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '英文关键词'] = \"\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '项目基金'] = \"教育部重大攻关课题“中国城市化理论重构与城市化发展战略”(课题项目批准号:05JZD0038)的成果之一;上海高校社会学E-研究院(上海大学)资助\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35727d1e-266f-49e1-b704-0779efe3d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.fillna('-1', inplace=True)\n",
    "merged_df = merged_df.replace(\"\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c425078-1903-450b-b628-00be48bff96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(merged_df, \"Article_Index\", outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3be20-b361-44f5-a60c-90bffcfaa5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0923ef61-87c2-45ad-a3f3-8ff490b8f3d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 下载文献原文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e603baf-97d9-4178-9855-e7e27aae9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../data/article_page/'\n",
    "outpath = '../data/pdf_file/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae922a4-a0e6-47be-81d9-cb7eb233f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = datapath+'Article_Index.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac6c83-3f81-485f-88f1-15a518fe850c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 补救措施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e454d6b-6566-488c-9133-14181f152a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 如果出错，找到出错的序号，重新运行此处及之后的代码\n",
    "# 找到ID为74380的行的索引\n",
    "start_index = df[df['ID'] == \"84242\"].index[0]\n",
    "\n",
    "# 取出ID为74380及以下的所有行\n",
    "df = df.iloc[start_index:]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecfd04f-30b8-481a-ba0c-ca58174f1f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件内容\n",
    "with open(outpath+'failed_ids.pkl', 'rb') as f:\n",
    "    bytes_data = f.read()\n",
    "\n",
    "# 将字节流反序列化为列表\n",
    "failed_ids = pickle.loads(bytes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c1d24-601d-4279-845a-724d4835e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##储存出错id\n",
    "#bytes_data = pickle.dumps(failed_ids)\n",
    "#with open(outpath+'failed_ids.pkl', 'wb') as f:\n",
    "#    f.write(bytes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c85bf9-057c-46e3-acc2-d8227f0bb11e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a7397-7daa-4b01-baf0-f2bf0ede3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历下载链接，下载文件\n",
    "failed_ids = []  # 用于存储下载失败的ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5149bd-4734-47ad-8132-4db447f58ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置日志记录\n",
    "logging.basicConfig(filename=outpath+'download_full_text.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 获取ID和下载链接列\n",
    "id_and_links = df[['ID', '下载链接']]\n",
    "\n",
    "# 初始化 Selenium WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "\n",
    "for index, row in tqdm(id_and_links.iterrows(), total=len(id_and_links), desc=\"Downloading\"):\n",
    "    article_id = row['ID']\n",
    "    download_link = row['下载链接']\n",
    "\n",
    "    if download_link != -1:\n",
    "        try:\n",
    "            # 使用Selenium获取文件名\n",
    "            driver.get(download_link)\n",
    "\n",
    "            # 获取文件扩展名，保留原有的扩展名\n",
    "            file_extension = urlparse(download_link).path.split('.')[-1]\n",
    "            \n",
    "            # 使用requests下载文件\n",
    "            response = requests.get(download_link, stream=True,timeout=40)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # 构造文件路径，使用文章ID和原有的扩展名\n",
    "            file_path = os.path.join(outpath, f'{article_id}.{file_extension}')\n",
    "            \n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            \n",
    "            logging.info(f\"Downloaded file for ID {article_id} and saved as {file_path}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error downloading file for ID {article_id}: {e}\")\n",
    "            failed_ids.append(article_id)  # 记录下载失败的ID\n",
    "            # 将字节流写入文件\n",
    "            bytes_data = pickle.dumps(failed_ids)\n",
    "            with open(outpath+'failed_ids.pkl', 'wb') as f:\n",
    "                f.write(bytes_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Other error downloading file for ID {article_id}: {e}\")\n",
    "            failed_ids.append(article_id)  # 记录下载失败的ID\n",
    "            # 将字节流写入文件\n",
    "            bytes_data = pickle.dumps(failed_ids)\n",
    "            with open(outpath+'failed_ids.pkl', 'wb') as f:\n",
    "                f.write(bytes_data)\n",
    "\n",
    "# 记录下载失败的ID到日志\n",
    "if failed_ids:\n",
    "    logging.warning(f\"Download failed for the following IDs: {failed_ids}\")\n",
    "\n",
    "# 关闭WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe1ead-24e1-42c0-98aa-53c66b3296fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40204010-d8b4-4f6e-9f4f-ede1b1fcced0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 检查下载结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0775df9-f98a-4924-89a4-e6b7cf69de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../data/article_page/'\n",
    "outpath = '../data/pdf_file/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02388e16-822c-4e64-b1f7-675b1f4d4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = datapath+'Article_Index.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2efc05-0fa2-463d-bdab-4ca1111dc589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取outpath下所有以.pdf结尾的文件\n",
    "pdf_files = [f for f in os.listdir(outpath) if f.endswith('.pdf')]\n",
    "\n",
    "# 生成文件名列表（不包括扩展名）\n",
    "succ = [os.path.splitext(f)[0] for f in pdf_files]\n",
    "\n",
    "# 从df中取出“ID”一列生成列表total\n",
    "total = df['ID'].tolist()\n",
    "\n",
    "# 将在total中但是不在succ中的元素组成列表fail\n",
    "fail = [id_ for id_ in total if id_ not in succ]\n",
    "\n",
    "# 打印结果\n",
    "print(\"fail:\", fail)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e44e492-da07-4e08-8023-107f8c362b40",
   "metadata": {},
   "source": [
    "手动下载：72707 72349\n",
    "文件不存在：\n",
    "    73393 外出打工与农村及农民发展——湖南省嘉禾县钟水村调查 (知网下载)\n",
    "    83008 《社会学研究》2022年第3期英文目录及摘要\n",
    "空白文档：\n",
    "    74103 全国社会保障支出按城乡分组的所有制构成（知网下载）\n",
    "    74095 全国社会保障费支出构成（知网下载）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cbe2d-b02a-47cc-a65d-b392d93d2eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d24a22d2-65c5-4afd-b271-2e6a412ba6bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 整理Article Index\n",
    "    1.删除全为缺失值的列“英文关键词”，“文章编号”\n",
    "    2.记录每个文档的页数，将pdf分为一页一页的文件，并重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c87a649d-a78e-47bb-a344-1981399c57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '../data/pdf_pages/'\n",
    "datapath = '../data/pdf_file/'\n",
    "indexpath = '../data/'\n",
    "samplepath = '../data/sample_pages/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb3396-2a50-4ba0-b03b-441900b4c771",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 开始整理索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b5bfe-2eb9-4623-9d90-82dde8371988",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = indexpath +'Article_Index.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33273d-4530-431d-a8de-dc2e06ee90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在原始DataFrame上删除'文章编号'和'英文关键词'两列，因为内容全是缺失值。\n",
    "df.drop(['文章编号', '英文关键词'], axis=1, inplace=True)\n",
    "\n",
    "# 打印修改后的DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1e8bd-3122-486a-adf6-9ad5023d4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_pages(pdf_path):\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_document:\n",
    "            return pdf_document.page_count\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_pdf_pages_dict(datapath):\n",
    "    pdf_pages_dict = {}\n",
    "    for filename in os.listdir(datapath):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_id = os.path.splitext(filename)[0]\n",
    "            pdf_path = os.path.join(datapath, filename)\n",
    "            pages = get_pdf_pages(pdf_path)\n",
    "            if pages is not None:\n",
    "                pdf_pages_dict[pdf_id] = pages\n",
    "    return pdf_pages_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83e914-d10d-42b3-9d52-eebb384757ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取PDF文件名（不带后缀）和页数的字典\n",
    "pdf_pages_dict = get_pdf_pages_dict(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9061037-9148-4c37-9235-264821ac4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将字典添加为新列到 DataFrame 中\n",
    "df['页数'] = df['ID'].map(pdf_pages_dict)\n",
    "\n",
    "# 使用 loc 方法将 'ID' 为 83008 的行的 '页数' 填充为 -1 这一文档不存在pdf文件，之前下载失败了。\n",
    "df.loc[df['ID'] == \"83008\", '页数'] = 0\n",
    "\n",
    "# 将这一列的格式转换为整数\n",
    "df['页数'] = df['页数'].astype(int)\n",
    "\n",
    "# 打印结果\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a4079-b225-40f7-8b35-9333b16b704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(df, \"Article_Index(final)\", indexpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6d248-5f5e-431a-83b3-076d336f0809",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 将每一个文件分为一页一页的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7fd2c3-512e-4cdf-8d75-bf50ab848e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = indexpath +'Article_Index(final).pkl'\n",
    "#df = pd.read_pickle(file_path)\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9bd48-d31c-401e-8750-645a00ff7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(input_path, output_path):\n",
    "    # 读取PDF文件\n",
    "    pdf_document = fitz.open(input_path)\n",
    "    total_pages = pdf_document.page_count\n",
    "\n",
    "    # 遍历每一页并保存成单独的文件\n",
    "    for page_num in range(total_pages):\n",
    "        pdf_writer = fitz.open()\n",
    "        pdf_writer.insert_pdf(pdf_document, from_page=page_num, to_page=page_num)\n",
    "\n",
    "        output_filename = f\"{os.path.splitext(os.path.basename(input_path))[0]}_{page_num + 1}.pdf\"\n",
    "        output_filepath = os.path.join(output_path, output_filename)\n",
    "\n",
    "        pdf_writer.save(output_filepath)\n",
    "        pdf_writer.close()\n",
    "\n",
    "def process_pdfs(datapath, outpath):\n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "    # 遍历datapath中的pdf文件，添加进度条\n",
    "    for filename in tqdm(os.listdir(datapath), desc=\"Processing PDFs\", unit=\"file\"):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            input_filepath = os.path.join(datapath, filename)\n",
    "            split_pdf(input_filepath, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a7daa-18bf-4525-89a4-e94d58f4ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pdfs(datapath, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53e7e7-acbd-4503-83cc-f4e3aab33012",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 检查是否被OCR过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599a811-e5bc-4522-80dc-97913651df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ocr_status(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        page = doc[0]\n",
    "        text = page.get_text(\"text\")\n",
    "        doc.close()\n",
    "        return 1 if text else 0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error checking OCR status for {pdf_path}: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307799b-3c4c-425c-b219-3ec2a3b2fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"../data/check_ocr/\"+'ocr_status.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "result_dict = {}\n",
    "files_without_ocr = []\n",
    "\n",
    "pdf_files = [filename for filename in os.listdir(outpath) if filename.endswith(\".pdf\")]\n",
    "\n",
    "for filename in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        pdf_path = os.path.join(outpath, filename)\n",
    "        ocr_status = check_ocr_status(pdf_path)\n",
    "        result_dict[filename] = ocr_status\n",
    "\n",
    "        if not ocr_status:\n",
    "            files_without_ocr.append(filename)\n",
    "\n",
    "logging.info(\"OCR Status:\")\n",
    "for filename, ocr_status in result_dict.items():\n",
    "    logging.info(f\"{filename}: {'OCR done' if ocr_status else 'Not OCR-ed'}\")\n",
    "    \n",
    "if files_without_ocr:\n",
    "    logging.info(\"\\nFiles without OCR:\")\n",
    "    for filename in files_without_ocr:\n",
    "        logging.info(filename)\n",
    "\n",
    "logging.info(f\"\\nTotal files without OCR: {len(files_without_ocr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793f20e-5f48-4aac-8d98-c925851f4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../data/check_ocr/\"+\"files_without_ocr.json\"\n",
    "\n",
    "save_to_json(files_without_ocr, json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e3103-405f-48c9-a12a-607f4773108e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 处理未OCR的页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252bdd9-5f22-4f52-bba5-5b5eb1da709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../data/check_ocr/\"+\"files_without_ocr.json\"\n",
    "files_without_ocr = read_from_json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f5c9a-62e3-4ba0-9b94-9fc5fff60473",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files_without_ocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d12af-aef7-4997-9bcb-d5fd970b3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标文件夹路径\n",
    "target_folder = '../data/check_ocr/un-ocr-ed/'\n",
    "# 确保目标文件夹存在，如果不存在则创建\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05188d75-27cb-40dd-99ad-adce0fb45abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置 logging，将日志输出到文件中\n",
    "logging.basicConfig(filename=target_folder+'file_transfer.log', level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "\n",
    "file_list = files_without_ocr\n",
    "\n",
    "# 配置 tqdm 进度条\n",
    "with tqdm(total=len(file_list), desc=\"文件转存进度\", unit=\"file\") as pbar:\n",
    "    # 遍历文件列表\n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith('.pdf'):  # 确保是PDF文件\n",
    "            source_path = os.path.join(outpath, file_name)\n",
    "            target_path = os.path.join(target_folder, file_name)\n",
    "\n",
    "            # 移动文件\n",
    "            shutil.move(source_path, target_path)\n",
    "\n",
    "            # 记录日志\n",
    "            logging.info(f\"文件 {file_name} 已成功转存到目标文件夹\")\n",
    "\n",
    "            # 更新进度条\n",
    "            pbar.update(1)\n",
    "\n",
    "print(\"文件已成功转存到目标文件夹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1f029-959d-4bd5-b2a1-c2e570026f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取ID，看一下有多少个没有ocr过的文件\n",
    "unique_ids = set()\n",
    "for file_name in file_list:\n",
    "    if file_name.endswith('.pdf'):  # 确保是PDF文件\n",
    "        # 假设ID和页码之间用下划线分割\n",
    "        file_id = file_name.split('_')[0]  # 提取ID\n",
    "        unique_ids.add(file_id)\n",
    "\n",
    "# 输出不重复的ID数量和ID列表\n",
    "print(f\"总共有 {len(unique_ids)} 个不重复的ID。\")\n",
    "print(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96d427-ac42-446b-a2a6-45719087d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = indexpath +'Final_Index.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfed3d3-1a07-4ab0-b588-88b72824b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要提取的ID列表\n",
    "ids_to_extract = list(unique_ids)\n",
    "\n",
    "# 使用isin方法选择特定ID的行，并选择 '年份'、'期数' 和 '标题' 列\n",
    "filtered_df = df[df['ID'].isin(ids_to_extract)][['年份', '期数', '标题']]\n",
    "\n",
    "# 按照年份、期数、标题依次排序\n",
    "sorted_df = filtered_df.sort_values(by=['年份', '期数', '标题'])\n",
    "\n",
    "# 输出结果\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25847a81-bb53-4147-93c0-b49b7d3ae493",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(sorted_df, \"Un-ocr-ed\", \"../data/check_ocr/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed88d0-69ff-43df-ab1b-4721ceb7e52d",
   "metadata": {},
   "source": [
    "    使用Adobe Acrobat 批量ocr页面\n",
    "    在outpath = '../data/pdf_pages/'中存储所有被ocr好的页面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a71fd2-6c8d-4246-ad43-c0a16d8cf984",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 为每一页单独建立索引 page_index\n",
    "    把索引按照年份拆分，避免单个文件行数过多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472a4385-c19a-41c2-b14f-16e9f89230e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the year blocks\n",
    "year_blocks = [(1986, 1989), (1990, 1994), (1995, 1999), (2000, 2004), (2005, 2009), (2010, 2014), (2015, 2019), (2020, 2024)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736a8c5-ee43-4eb1-87e6-37f3e575de93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 根据年份划分索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0536467-d245-4929-a303-8047cbeb0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = indexpath +'Article_Index(final).pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "df['年份'] = df['年份'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3749ea1-9966-4927-ba50-46b9d666dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame 'pages' with the same columns as df and add two new columns '页码' and 'ocr'\n",
    "columns = df.columns.tolist() + ['页码', 'ocr']\n",
    "\n",
    "# Process each block\n",
    "for start_year, end_year in year_blocks:\n",
    "    # Filter the dataframe for the current block\n",
    "    df_block = df[(df['年份'] >= start_year) & (df['年份'] <= end_year)]\n",
    "    pages = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # For each row in the block, repeat the row '页数' times and add to 'pages'\n",
    "    for _, row in df_block.iterrows():\n",
    "        repeat_times = row['页数']\n",
    "        for page in range(1, repeat_times + 1):\n",
    "            new_row = row.to_dict()\n",
    "            new_row['页码'] = page\n",
    "            new_row['ocr'] = None  # Set 'ocr' as empty\n",
    "            pages = pages.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # save\n",
    "    suffix = ''.join(['_',str(start_year),'-',str(end_year)])\n",
    "    save_dataframe(pages, \"page_index\", indexpath,suffix)\n",
    "    print(suffix,\"saved.\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfd62d-643d-42ff-8034-436260d68d0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 填充‘ocr’一列\n",
    "    标记是否用Adobe acrobat进行过ocr（ocr，1表示是，0表示本来就已经经过ocr）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a676021-fc5d-422a-ae62-74078dcf081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../data/check_ocr/\"+\"files_without_ocr.json\"\n",
    "files_without_ocr = read_from_json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f402e48-b7a6-4f2f-81b1-b7485912b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_without_ocr = [item.replace('.pdf', '') for item in files_without_ocr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524eeee0-1262-41b3-8e0b-75e560421356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_ocr(file_name, indexpath, files_without_ocr):\n",
    "    # Construct the full path\n",
    "    full_path = indexpath + file_name\n",
    "    \n",
    "    # Load the DataFrame using pd.read_pickle\n",
    "    df = pd.read_pickle(full_path)\n",
    "    \n",
    "    # Initialize counters for '1's and '0's\n",
    "    count_1s = 0\n",
    "    count_0s = 0\n",
    "    \n",
    "    # Iterate over each row in DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Construct the string s = ID_页码\n",
    "        s = f\"{row['ID']}_{row['页码']}\"\n",
    "        \n",
    "        # Check if s is in files_without_ocr_list and update 'ocr' column accordingly\n",
    "        if s in files_without_ocr:\n",
    "            df.at[index, 'ocr'] = 1\n",
    "            count_1s += 1\n",
    "        else:\n",
    "            df.at[index, 'ocr'] = 0\n",
    "            count_0s += 1\n",
    "            \n",
    "    # This print statement confirms the function call and shows the counts\n",
    "    print(f\"Processed {file_name}:\")\n",
    "    print(f\"    Count of '1's: {count_1s}\")\n",
    "    print(f\"    Count of '0's: {count_0s}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c13ee7-dcc9-4caa-bbd5-987d97b595b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the year blocks to construct file names with the path, load, and process DataFrames\n",
    "for i in range(len(year_blocks)):\n",
    "    file_name = f\"page_index_{year_blocks[i][0]}-{year_blocks[i][1]}.pkl\"\n",
    "    df_filled = fill_ocr(file_name, indexpath, files_without_ocr)\n",
    "    # save\n",
    "    fl_name = f\"page_index_{year_blocks[i][0]}-{year_blocks[i][1]}\"\n",
    "    save_dataframe(df_filled, fl_name, indexpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f35381-eabc-4fce-a2ee-1118d9e8f01e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 建立pickle格式的总目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc868d0-d6c3-4c4e-9215-a7b4d9bf2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个空的DataFrame作为起始点\n",
    "merged_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65cbc374-c112-4d08-baa9-f4df61934be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the year blocks to construct file names with the path, load, and process DataFrames\n",
    "for i in range(len(year_blocks)):\n",
    "    file_name = f\"page_index_{year_blocks[i][0]}-{year_blocks[i][1]}.pkl\"\n",
    "    full_path = indexpath + file_name\n",
    "    df = pd.read_pickle(full_path)\n",
    "    merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "# 重置索引，并且不保留旧索引\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 将合并后的DataFrame保存为Pickle文件\n",
    "merged_df.to_pickle(indexpath+\"page_index.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
