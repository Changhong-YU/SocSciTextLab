{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2165da88-dd3d-4f31-8475-1437378935fe",
   "metadata": {},
   "source": [
    "# 下载《社会学研究》期刊文章"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4805c-1bad-43bf-a043-71a2ab12ba3f",
   "metadata": {},
   "source": [
    "## 导入所需的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a13d2f-361d-41a9-9057-995ab273b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d6801-009e-4196-9986-249efdadc9f7",
   "metadata": {},
   "source": [
    "## 给出文件存储目录、常用函数和webdriver的基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1c3ba-1284-433f-b5e0-cb344c2bf27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '../data/'\n",
    "datapath = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fb917-ae7c-439a-a515-e11039dcc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, file, outpath,suffix=\"\"):\n",
    "    # 创建最终的文件名\n",
    "    filename = f\"{file}{suffix}\"\n",
    "\n",
    "    # 存为 Excel 文件（使用 utf-16 编码）\n",
    "    excel_filepath = os.path.join(outpath, f\"{filename}.xlsx\")\n",
    "    df.to_excel(excel_filepath, index=False, encoding='utf-16')\n",
    "\n",
    "    # 存为 Python 二进制文件（Pickle 格式，也使用 utf-16 编码）\n",
    "    pickle_filepath = os.path.join(outpath, f\"{filename}.pkl\")\n",
    "    df.to_pickle(pickle_filepath, protocol=5)  # protocol=5 表示使用最高的 Pickle 协议版本\n",
    "    \n",
    "    #print(excel_filepath, 'saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3d6f8-b6ab-424f-8edd-db21973dc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver输入参数\n",
    "options=webdriver.ChromeOptions()\n",
    "out_path=os.path.abspath(outpath)\n",
    "prefs={'profile.default_content_settings.popups': 0, 'download.default_directory': out_path}\n",
    "options.add_experimental_option('prefs', prefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817aa88b-e136-4686-ad4d-db857a17cd90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 下载指定年份的期刊目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aedbc5-f4c3-407e-b1d0-316961d6ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '../data/issue_page/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efd30d-5b0b-4829-9622-65f8340fc2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(prompt):\n",
    "    for _ in range(5):\n",
    "        year = input(prompt)\n",
    "        now = time.strftime(\"%Y%m%d_%H%M%S\")[:4]\n",
    "        if re.match(r\"^\\d+$\", year) and int(year) >= 1986 and int(year) <= int(now):\n",
    "            return int(year)\n",
    "        else:\n",
    "            print(\"无效的年份！请重新输入：\")\n",
    "    print(\"输入年份超过五次错误，程序终止！\")\n",
    "    exit()\n",
    "\n",
    "def year_range():\n",
    "    now = time.strftime(\"%Y%m%d_%H%M%S\")[:4]\n",
    "    start_year = get_year(''.join([\"请输入起始年份（1986 <= start yr <=\",now,\"）：\"]))\n",
    "    end_year = get_year(''.join([\"请输入结束年份（1986 <= end yr <=\",now,\"）：\"]))\n",
    "\n",
    "    # 检查结束年份是否有效\n",
    "    while end_year < start_year:\n",
    "        print(\"结束年份必须大于或等于起始年份！\")\n",
    "        end_year = get_year(\"请输入结束年份：\")\n",
    "    return start_year, end_year\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602296c5-41fa-471d-9178-61258dcd3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year, end_year = year_range()\n",
    "print(\"起始年份：\", start_year)\n",
    "print(\"结束年份：\", end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647ec52-8f19-4f48-ab15-1723a76c3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化logging模块\n",
    "logging.basicConfig(filename=outpath+'download_'start_year+'_'+end_year+'.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# 记录脚本开始运行的时间\n",
    "start_script_time = time.time()\n",
    "logging.info(\"Script started\")\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "\n",
    "successful_pages = []\n",
    "failed_pages = []\n",
    "\n",
    "for yr in range(start_year, end_year + 1):\n",
    "    for issue_num in range(1, 7):\n",
    "        issue_url = f\"http://shxyj.ajcass.org/Magazine/?Year={yr}&Issue={issue_num}\"\n",
    "        \n",
    "        try:\n",
    "            driver.get(issue_url)\n",
    "            try:\n",
    "                driver.find_element_by_css_selector(\"table#tab tr\")\n",
    "                logging.info(f\"Page for Year {yr}, Issue {issue_num} has content.\")\n",
    "                successful_pages.append(issue_url)\n",
    "            except NoSuchElementException:\n",
    "                logging.warning(f\"Page for Year {yr}, Issue {issue_num} does not have content.\")\n",
    "                failed_pages.append(issue_url)\n",
    "                    \n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'html')))\n",
    "            \n",
    "            issue_content = driver.page_source\n",
    "            fl_name = f\"{yr}_{issue_num}\"\n",
    "            with open(f'{outpath}/{fl_name}.html', 'w', encoding='utf-8') as file:\n",
    "                file.write(issue_content)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to download the page for Year {yr} Issue {issue_num}. Error: {e}\")\n",
    "            failed_pages.append(issue_url)\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# 记录脚本结束运行的时间\n",
    "end_script_time = time.time()\n",
    "logging.info(\"Script ended\")\n",
    "\n",
    "# 计算脚本运行总时长\n",
    "total_time = end_script_time - start_script_time\n",
    "logging.info(f\"Total script run time: {total_time} seconds\")\n",
    "\n",
    "# 记录成功和失败的页面\n",
    "logging.info(f\"Successful pages: {successful_pages}\")\n",
    "logging.info(f\"Failed pages: {failed_pages}\")\n",
    "\n",
    "# 记录开始和结束的年份\n",
    "logging.info(f\"Start year: {start_year}, End year: {end_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7040996-dc69-4d5b-86d4-c5dc419aa28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缺少1996 issue-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b22d6-552b-419f-bdf5-acb2f2a426e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 清理出期刊总目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10599c4b-3eb5-4621-afa8-f592f4b41a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '../data/issue_page/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd68490-6b35-4778-940c-2a8be65e5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articles_to_dataframe(html_file_path):\n",
    "    # 读取HTML文件\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    data = []\n",
    "\n",
    "    # 解析HTML，提取文章信息\n",
    "    for tr in soup.select('table#tab tr'):\n",
    "        td = tr.find_all('td')\n",
    "        if len(td) > 1:\n",
    "            ul = td[1].find('ul')\n",
    "            if ul:\n",
    "                li_items = ul.find_all('li', recursive=False)\n",
    "                if len(li_items) >= 4:\n",
    "                    title = li_items[0].text.strip()\n",
    "                    abstract = li_items[1].text.strip()[len('[摘要]'):].strip() if li_items[1].text.startswith('[摘要]') else '无摘要'\n",
    "                    author = li_items[2].text.strip().replace('作者：', '').replace('\\n', ', ').strip()\n",
    "                    views = li_items[3].text.strip().split(' ')[-2] if li_items[3].text.strip().split(' ') else '0'\n",
    "                    article_url = li_items[0].find('a', href=True)['href'] if li_items[0].find('a', href=True) else '无文章链接'\n",
    "\n",
    "                    data.append([title, abstract, author, views, article_url])\n",
    "\n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(data, columns=['标题', '摘要', '作者', '浏览次数', '文章URL'])\n",
    "    \n",
    "    # 提取ID并更新URL\n",
    "    df['ID'] = df['文章URL'].str.extract(r'(\\d+)$')\n",
    "    df['文章URL'] = 'http://shxyj.ajcass.org/' + df['文章URL']\n",
    "\n",
    "    # 调整列顺序\n",
    "    cols = ['ID'] + [col for col in df if col != 'ID']\n",
    "    df = df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe6329-2382-4383-9c8c-6b02a1df38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收集目录中所有的HTML文件名\n",
    "files = [f for f in os.listdir(outpath) if f.endswith('.html')]\n",
    "\n",
    "# 提取年份和期数，转换为整数，用于排序\n",
    "files_sorted = sorted(files, key=lambda x: [int(part) for part in x.split('.')[0].split('_')])\n",
    "\n",
    "# 初始化一个空的DataFrame来存储所有数据\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# 按排序后的顺序遍历和处理每个文件\n",
    "for filename in files_sorted:\n",
    "    file_path = os.path.join(outpath, filename)\n",
    "    year, issue = filename.split('.')[0].split('_')\n",
    "    df = extract_articles_to_dataframe(file_path)\n",
    "    df.insert(1, '年份', year)\n",
    "    df.insert(2, '期数', issue)\n",
    "    all_data = pd.concat([all_data, df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20e8df-5c2f-466f-974f-c750efef1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(all_data, \"master_index\", outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a5a8c-1dbf-4ea3-ab75-5f75845335cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3171b0d-8734-4bb8-b2f3-d018742c6f15",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 添加每篇文章的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9d637-6907-4018-a18e-d77c176a171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../data/issue_page/'\n",
    "outpath = '../data/article_page/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69ae06-3a72-445f-ac0c-d71239f7d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = datapath+'master_index.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c1e7c8-b866-42b8-a3fe-fe0afa6300f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 下载每篇文章的介绍页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8057a0-70fc-4c24-8571-d7bcba4b82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置日志配置\n",
    "logging.basicConfig(filename=outpath+'article_info.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 记录起始时间\n",
    "start_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "logging.info(f\"开始执行下载任务，起始时间: {start_time}\")\n",
    "\n",
    "# 初始化 Selenium WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "\n",
    "# 创建列表来存储下载失败的文章ID\n",
    "failed_articles = []\n",
    "\n",
    "# 遍历 DataFrame\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"下载进度\"):\n",
    "    # 获取 ID 和文章 URL\n",
    "    article_id = row['ID']\n",
    "    article_url = row['文章URL']\n",
    "    \n",
    "    try:\n",
    "        # 使用 Selenium 下载网页\n",
    "        driver.get(article_url)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'html')))\n",
    "        article_content = driver.page_source\n",
    "        \n",
    "        # 保存网页内容到文件\n",
    "        with open(f'{outpath}{article_id}.html', 'w', encoding='utf-8') as file:\n",
    "            file.write(article_content)\n",
    "        \n",
    "        # 记录成功下载\n",
    "        logging.info(f\"成功下载文章 {article_id}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # 记录下载失败\n",
    "        logging.error(f\"下载文章 {article_id} 失败: {e}\")\n",
    "        # 将失败的文章ID添加到列表中\n",
    "        failed_articles.append(article_id)\n",
    "    \n",
    "# 关闭 WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# 记录截止时间\n",
    "end_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "logging.info(f\"下载任务执行完成，截止时间: {end_time}\")\n",
    "\n",
    "# 记录统计信息\n",
    "logging.info(f\"总文章数: {len(df)}\")\n",
    "logging.info(f\"成功下载文章数: {len(df)-len(failed_articles)}\")\n",
    "logging.info(f\"下载失败文章数: {len(failed_articles)}\")\n",
    "\n",
    "# 记录所有失败的文章ID\n",
    "if failed_articles:\n",
    "    logging.warning(f\"以下文章下载失败: {failed_articles}\")\n",
    "else:\n",
    "    logging.info(\"所有文章下载成功。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c7bd8-bb40-40d3-ba28-76d3595e6e81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 继续提取信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad589f6-7601-45df-8150-bb694af491b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数来提取网页中的信息\n",
    "def extract_info(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # 提取英文标题\n",
    "        english_title = soup.select_one('td:contains(\"英文标题\") + td').text.strip()\n",
    "\n",
    "        # 提取英文摘要\n",
    "        english_abstract = soup.select_one('td:contains(\"英文摘要\") + td').text.strip()\n",
    "\n",
    "        # 提取作者单位\n",
    "        author_affiliation = soup.select_one('td:contains(\"作者单位\") + td').text.strip()\n",
    "\n",
    "        # 提取期刊\n",
    "        journal = soup.select_one('td:contains(\"期刊\") + td a').text.strip()\n",
    "\n",
    "        # 提取年.期:页码\n",
    "        issue_info = soup.select_one('td:contains(\"年.期:页码\") + td').text.strip()\n",
    "\n",
    "        # 提取中图分类号\n",
    "        classification = soup.select_one('td:contains(\"中图分类号\") + td').text.strip()\n",
    "\n",
    "        # 提取关键词\n",
    "        keywords = soup.select_one('td:contains(\"关键词\") + td').text.strip()\n",
    "\n",
    "        # 提取英文关键词\n",
    "        english_keywords = soup.select_one('td:contains(\"英文关键词\") + td').text.strip()\n",
    "\n",
    "        # 提取项目基金\n",
    "        project_fund = soup.select_one('td:contains(\"项目基金\") + td').text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting information from {file_path}: {str(e)}\")\n",
    "        #return {key: '-1' for key in ['英文标题', '英文摘要', '作者单位', '期刊', '年.期:页码', '中图分类号', '关键词', '英文关键词', '项目基金']}\n",
    "    \n",
    "    try:\n",
    "        # 提取下载链接\n",
    "        download_link = 'http://shxyj.ajcass.org' + soup.find('a', href=re.compile(r'^/Admin/UploadFile/'))['href']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting information from {file_path}: {str(e)}\")\n",
    "        #return {key: '-1' for key in ['英文标题', '英文摘要', '作者单位', '期刊', '年.期:页码', '中图分类号', '关键词', '英文关键词', '项目基金']}\n",
    "    \n",
    "    try:\n",
    "        # 创建一个字典用于构建DataFrame\n",
    "        info_dict = {\n",
    "            '英文标题': english_title if 'english_title' in locals() else '-1',\n",
    "            '英文摘要': english_abstract if 'english_abstract' in locals() else '-1',\n",
    "            '作者单位': author_affiliation if 'author_affiliation' in locals() else '-1',\n",
    "            '期刊': journal if 'journal' in locals() else '-1',\n",
    "            '年.期:页码': issue_info if 'issue_info' in locals() else '-1',\n",
    "            '中图分类号': classification if 'classification' in locals() else '-1',\n",
    "            '关键词': keywords if 'keywords' in locals() else '-1',\n",
    "            '英文关键词': english_keywords if 'english_keywords' in locals() else '-1',\n",
    "            '项目基金': project_fund if 'project_fund' in locals() else '-1',\n",
    "            '下载链接': download_link if 'download_link' in locals() else '-1'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting information from {file_path}: {str(e)}\")\n",
    "        return {key: '-1' for key in ['英文标题', '英文摘要', '作者单位', '期刊', '年.期:页码', '中图分类号', '关键词', '英文关键词', '项目基金']}\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16899204-1ee1-4f60-a325-79e6e58e2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 logging\n",
    "logging.basicConfig(filename=outpath+'Article_Index_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 设置 tqdm 进度条\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# 创建一个列表，用于存储每篇文章的信息\n",
    "articles_info = []\n",
    "\n",
    "# 遍历文件夹中的网页文件\n",
    "for article_id in tqdm(df['ID'], desc='Processing Articles'):\n",
    "    file_name = f'{article_id}.html'\n",
    "    file_path = os.path.join(outpath, file_name)\n",
    "\n",
    "    # 提取信息并添加到列表中\n",
    "    info_dict = extract_info(file_path)\n",
    "    info_dict['ID']= article_id\n",
    "\n",
    "    if info_dict:\n",
    "        articles_info.append(info_dict)\n",
    "\n",
    "# 将列表转换为DataFrame\n",
    "articles_info_df = pd.DataFrame(articles_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6a0d5-8072-474a-b99e-8493e3a4a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将两个DataFrame连接在一起，按照原有的df['ID']列连接\n",
    "merged_df = pd.merge(df, articles_info_df, on='ID', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10236acc-c078-48ba-ac12-dfc12549d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动修正出错的单元格\n",
    "merged_df.loc[merged_df['ID'] == '72349', '英文标题'] = \"State , Market and Society: The multidimensional impetus of reducing the Qinhuai River's pollution\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '英文摘要'] = \"Since 1950, the Qinhuai River in Nanjing city has undergone five times of large-scale renovations. ... (以下是英文摘要的修改内容)\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '作者单位'] = \"南京大学社会学系 上海高校社会学E-研究院(上海大学)\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '期刊'] = \"社会学研究\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '年.期:页码'] = \"2008.1:143-164\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '中图分类号'] = \"X321\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '文章编号'] = \"\"  # 更新文章编号为空字符串\n",
    "merged_df.loc[merged_df['ID'] == '72349', '关键词'] = \"国家； 市场与社会； 污染治理； 中国特色；\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '英文关键词'] = \"\"\n",
    "merged_df.loc[merged_df['ID'] == '72349', '项目基金'] = \"教育部重大攻关课题“中国城市化理论重构与城市化发展战略”(课题项目批准号:05JZD0038)的成果之一;上海高校社会学E-研究院(上海大学)资助\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35727d1e-266f-49e1-b704-0779efe3d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.fillna('-1', inplace=True)\n",
    "merged_df = merged_df.replace(\"\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c425078-1903-450b-b628-00be48bff96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(merged_df, \"Article_Index\", outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3be20-b361-44f5-a60c-90bffcfaa5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0923ef61-87c2-45ad-a3f3-8ff490b8f3d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 下载文献原文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e603baf-97d9-4178-9855-e7e27aae9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../data/article_page/'\n",
    "outpath = '../data/pdf_file/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae922a4-a0e6-47be-81d9-cb7eb233f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = datapath+'Article_Index.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac6c83-3f81-485f-88f1-15a518fe850c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 补救措施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e454d6b-6566-488c-9133-14181f152a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 如果出错，找到出错的序号，重新运行此处及之后的代码\n",
    "# 找到ID为74380的行的索引\n",
    "start_index = df[df['ID'] == \"84242\"].index[0]\n",
    "\n",
    "# 取出ID为74380及以下的所有行\n",
    "df = df.iloc[start_index:]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecfd04f-30b8-481a-ba0c-ca58174f1f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件内容\n",
    "with open(outpath+'failed_ids.pkl', 'rb') as f:\n",
    "    bytes_data = f.read()\n",
    "\n",
    "# 将字节流反序列化为列表\n",
    "failed_ids = pickle.loads(bytes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c1d24-601d-4279-845a-724d4835e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##储存出错id\n",
    "#bytes_data = pickle.dumps(failed_ids)\n",
    "#with open(outpath+'failed_ids.pkl', 'wb') as f:\n",
    "#    f.write(bytes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c85bf9-057c-46e3-acc2-d8227f0bb11e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a7397-7daa-4b01-baf0-f2bf0ede3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历下载链接，下载文件\n",
    "failed_ids = []  # 用于存储下载失败的ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5149bd-4734-47ad-8132-4db447f58ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置日志记录\n",
    "logging.basicConfig(filename=outpath+'download_full_text.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 获取ID和下载链接列\n",
    "id_and_links = df[['ID', '下载链接']]\n",
    "\n",
    "# 初始化 Selenium WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "\n",
    "for index, row in tqdm(id_and_links.iterrows(), total=len(id_and_links), desc=\"Downloading\"):\n",
    "    article_id = row['ID']\n",
    "    download_link = row['下载链接']\n",
    "\n",
    "    if download_link != -1:\n",
    "        try:\n",
    "            # 使用Selenium获取文件名\n",
    "            driver.get(download_link)\n",
    "\n",
    "            # 获取文件扩展名，保留原有的扩展名\n",
    "            file_extension = urlparse(download_link).path.split('.')[-1]\n",
    "            \n",
    "            # 使用requests下载文件\n",
    "            response = requests.get(download_link, stream=True,timeout=40)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # 构造文件路径，使用文章ID和原有的扩展名\n",
    "            file_path = os.path.join(outpath, f'{article_id}.{file_extension}')\n",
    "            \n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            \n",
    "            logging.info(f\"Downloaded file for ID {article_id} and saved as {file_path}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error downloading file for ID {article_id}: {e}\")\n",
    "            failed_ids.append(article_id)  # 记录下载失败的ID\n",
    "            # 将字节流写入文件\n",
    "            bytes_data = pickle.dumps(failed_ids)\n",
    "            with open(outpath+'failed_ids.pkl', 'wb') as f:\n",
    "                f.write(bytes_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Other error downloading file for ID {article_id}: {e}\")\n",
    "            failed_ids.append(article_id)  # 记录下载失败的ID\n",
    "            # 将字节流写入文件\n",
    "            bytes_data = pickle.dumps(failed_ids)\n",
    "            with open(outpath+'failed_ids.pkl', 'wb') as f:\n",
    "                f.write(bytes_data)\n",
    "\n",
    "# 记录下载失败的ID到日志\n",
    "if failed_ids:\n",
    "    logging.warning(f\"Download failed for the following IDs: {failed_ids}\")\n",
    "\n",
    "# 关闭WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe1ead-24e1-42c0-98aa-53c66b3296fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40204010-d8b4-4f6e-9f4f-ede1b1fcced0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 检查下载结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0775df9-f98a-4924-89a4-e6b7cf69de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../data/article_page/'\n",
    "outpath = '../data/pdf_file/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02388e16-822c-4e64-b1f7-675b1f4d4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = datapath+'Article_Index.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2efc05-0fa2-463d-bdab-4ca1111dc589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取outpath下所有以.pdf结尾的文件\n",
    "pdf_files = [f for f in os.listdir(outpath) if f.endswith('.pdf')]\n",
    "\n",
    "# 生成文件名列表（不包括扩展名）\n",
    "succ = [os.path.splitext(f)[0] for f in pdf_files]\n",
    "\n",
    "# 从df中取出“ID”一列生成列表total\n",
    "total = df['ID'].tolist()\n",
    "\n",
    "# 将在total中但是不在succ中的元素组成列表fail\n",
    "fail = [id_ for id_ in total if id_ not in succ]\n",
    "\n",
    "# 打印结果\n",
    "print(\"fail:\", fail)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e44e492-da07-4e08-8023-107f8c362b40",
   "metadata": {},
   "source": [
    "手动下载：72707 72349\n",
    "文件不存在：\n",
    "    73393 外出打工与农村及农民发展——湖南省嘉禾县钟水村调查 (知网下载)\n",
    "    83008 《社会学研究》2022年第3期英文目录及摘要\n",
    "空白文档：\n",
    "    74103 全国社会保障支出按城乡分组的所有制构成（知网下载）\n",
    "    74095 全国社会保障费支出构成"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
